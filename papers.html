---
layout: page
title: Papers
---

<section id="papers">
<div class="container">

<h3>{{ page.title }}</h3>
<div class="panel panel-default">
  <div class="panel-body">
    <h5>
      <strong><a href="https://arxiv.org/abs/1804.04539">Generative Visual Rationales</a></strong>
    </h5>
    <p>
      Interpretability and small labelled datasets are key issues in the practical application of deep learning, particularly in areas such as medicine. In this paper, we present a semi-supervised technique that addresses both these issues by leveraging large unlabelled datasets to encode and decode images into a dense latent representation. Using chest radiography as an example, we apply this encoder to other labelled datasets and apply simple models to the latent vectors to learn algorithms to identify heart failure.
    </p>
    <p>
      <a href="https://arxiv.org/abs/1804.04539">arXiv</a> |
      <a href="https://arxiv.org/pdf/1804.04539">pdf</a>
    </p>

    <h5>
      <strong><a href="https://arxiv.org/abs/1708.00129
">Deep Generative Adversarial Neural Networks for Realistic Prostate Lesion MRI Synthesis
</a></strong>
    </h5>
    <p>
      Generative Adversarial Neural Networks (GANs) are applied to the synthetic generation of prostate lesion MRI images. GANs have been applied to a variety of natural images, is shown show that the same techniques can be used in the medical domain to create realistic looking synthetic lesion images. 16mm x 16mm patches are extracted from 330 MRI scans from the SPIE ProstateX Challenge 2016 and used to train a Deep Convolutional Generative Adversarial Neural Network (DCGAN) utilizing cutting edge techniques. Synthetic outputs are compared to real images and the implicit latent representations induced by the GAN are explored. Training techniques and successful neural network architectures are explained in detail.
    </p>
    <p>
      <a href="https://arxiv.org/abs/1708.00129">arXiv</a> |
      <a href="https://arxiv.org/pdf/1708.00129">pdf</a>
    </p>

  </div>
</div>

</div>
</section>
